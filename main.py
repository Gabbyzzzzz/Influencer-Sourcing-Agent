import os
import re
import json
import pandas as pd
from dotenv import load_dotenv
from google import genai
from googleapiclient.discovery import build
from playwright.sync_api import sync_playwright

# --- 1. é…ç½®åŠ è½½ ---
load_dotenv()
client = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
SEARCH_ENGINE_ID = os.getenv("SEARCH_ENGINE_ID")


# --- 2. æ ¸å¿ƒå·¥å…·å‡½æ•° ---

def google_search(query):
    try:
        service = build("customsearch", "v1", developerKey=GOOGLE_API_KEY)
        res = service.cse().list(q=query, cx=SEARCH_ENGINE_ID, num=10).execute()  # æ¯æ¬¡æœ10ä¸ª
        return res.get('items', [])
    except Exception as e:
        print(f"âŒ æœç´¢å‡ºé”™: {e}")
        return []


def smart_scrape(url):
    """ ä½¿ç”¨ Playwright æŠ“å–å†…å®¹ """
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            context = browser.new_context(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
            page = context.new_page()
            page.goto(url, wait_until="domcontentloaded", timeout=30000)
            text = page.inner_text("body")
            browser.close()
            return " ".join(text.split())[:3000]
    except:
        return ""


def deep_analyze(brand_req, page_content):
    """ AI è¯„ä¼°é€»è¾‘ """
    prompt = f"å“ç‰Œéœ€æ±‚ï¼š{brand_req}\nå†…å®¹ï¼š{page_content}\nåˆ†æå¹¶è¾“å‡ºJSONï¼š{{'name':'','score':0,'contact':'','tags':[],'reason':''}}"
    try:
        response = client.models.generate_content(model="gemini-2.0-flash", contents=prompt)
        json_match = re.search(r'\{.*\}', response.text, re.DOTALL)
        return json.loads(json_match.group(0)) if json_match else None
    except:
        return None


# --- 3. æ™ºèƒ½ Agent ä¸»é€»è¾‘ ---

def run_smart_agent():
    brand_req = "æ¨å¹¿ä¸€æ¬¾é€‚åˆç¨‹åºå‘˜çš„é™éŸ³ã€äººä½“å·¥å­¦æœºæ¢°é”®ç›˜"

    # --- A. è®© AI è‡ªåŠ¨ç”Ÿæˆå¤šä¸ªæœç´¢ç»´åº¦ ---
    print("ğŸ§  Agent æ­£åœ¨æ€è€ƒå¤šä¸ªæœç´¢è§’åº¦...")
    plan_prompt = f"é’ˆå¯¹éœ€æ±‚ '{brand_req}'ï¼Œç»™å‡º3ä¸ªäº’ä¸ç›¸åŒçš„ Google æœç´¢æŒ‡ä»¤ã€‚ä¾‹å¦‚ä¸€ä¸ªé’ˆå¯¹YouTubeï¼Œä¸€ä¸ªé’ˆå¯¹ä¸“ä¸šç§‘æŠ€åª’ä½“ï¼Œä¸€ä¸ªé’ˆå¯¹ä¸ªäººåšå®¢ã€‚åªè¾“å‡ºæœç´¢æŒ‡ä»¤ï¼Œæ¯è¡Œä¸€ä¸ªã€‚"
    plan_res = client.models.generate_content(model="gemini-2.0-flash", contents=plan_prompt)
    search_queries = [q.strip() for q in plan_res.text.strip().split('\n') if q.strip()]

    print(f"ğŸ“‹ ç¡®å®šçš„æœç´¢è®¡åˆ’ï¼š\n{search_queries}\n")

    all_data = []
    visited_urls = set()  # ç”¨äºè®°å½•å·²ç»å¤„ç†è¿‡çš„é“¾æ¥ï¼Œé˜²æ­¢é‡å¤

    # --- B. å¾ªç¯æ‰§è¡Œæœç´¢è®¡åˆ’ ---
    for idx, query in enumerate(search_queries):
        print(f"--- ğŸš€ æ­£åœ¨æ‰§è¡Œç¬¬ {idx + 1} è½®æœç´¢: {query} ---")
        raw_items = google_search(query)

        for item in raw_items:
            url = item['link']

            # ğŸ’¡ å…³é”®ï¼šå»é‡æ£€æŸ¥
            if url in visited_urls:
                continue
            visited_urls.add(url)

            print(f"ğŸ” æ­£åœ¨å¤„ç†: {item['title'][:30]}...")
            content = smart_scrape(url)

            if content:
                analysis = deep_analyze(brand_req, content)
                if analysis and analysis.get('score', 0) >= 6:  # åªè¦ 6 åˆ†ä»¥ä¸Šçš„ä¼˜è´¨ç½‘çº¢
                    analysis['link'] = url
                    all_data.append(analysis)
                    print(f"   âœ… å‘ç°åŒ¹é…åšä¸»: {analysis['name']} (å¾—åˆ†: {analysis['score']})")

    # --- C. æœ€ç»ˆæ±‡æ€»ä¿å­˜ ---
    if all_data:
        df = pd.DataFrame(all_data)
        # æŒ‰ç…§åˆ†æ•°ä»é«˜åˆ°ä½æ’åº
        df = df.sort_values(by="score", ascending=False)
        df.to_csv("master_influencer_list.csv", index=False, encoding="utf-8-sig")
        print(f"\nğŸ‰ å¤§åŠŸå‘Šæˆï¼å…±æœé›†åˆ° {len(all_data)} ä½ä¼˜è´¨å€™é€‰äººï¼Œå·²å­˜å…¥ master_influencer_list.csv")
    else:
        print("\nğŸ˜” æœªèƒ½æ‰¾åˆ°åŒ¹é…çš„åšä¸»ï¼Œè¯·å°è¯•è°ƒæ•´å“ç‰Œéœ€æ±‚å…³é”®è¯ã€‚")


if __name__ == "__main__":
    run_smart_agent()